
# Emotion Detection from Speech â€“ Transformer-Based Model

This project implements a transformer-based model for emotion recognition from speech signals, built entirely from scratch without relying on pre-trained libraries like HuggingFace.

## ğŸ” Overview

The system classifies emotional states from speech using mel-spectrogram features and a transformer encoder architecture. It has been tested with two datasets:
- [RAVDESS](https://zenodo.org/record/1188976)
- [CREMA-D](https://www.kaggle.com/datasets/ejlok1/cremad)

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ AudioWAV/                 # Raw CREMA-D audio files
â”‚   â””â”€â”€ ravdess/                  # Raw RAVDESS audio files
â”‚
â”œâ”€â”€ datasetmetadata/
â”‚   â””â”€â”€ ravdess_metadata.csv      # Metadata for RAVDESS
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ training_curves_cremad.png
â”‚   â”œâ”€â”€ training_curves_ravdess.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ data_dist_cremad.png
â”‚   â”œâ”€â”€ multi_head_attention.png
â”‚   â””â”€â”€ transformer_from_scratch.png
â”‚   â””â”€â”€ example_output.png
â”‚   â””â”€â”€ model_summary.png
â”‚
â”œâ”€â”€ model/ #contains model weights
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ create_mel_spectrogram.py
â”‚   â”œâ”€â”€ dataset.py                # Dataset loaders for RAVDESS & CREMA-D
â”‚   â”œâ”€â”€ data_dist_cremad.py      # Class distribution visualization
â”‚   â”œâ”€â”€ generate_data_files_for_input.py
â”‚   â”œâ”€â”€ generate_cremad_metadata.py
â”‚   â””â”€â”€ visualize.py              # Training & evaluation visualizations
â”‚
â”œâ”€â”€ train.py                      # Main training script
â”œâ”€â”€ inference.py                  # Real-time microphone inference
â”‚â”€â”€ model.py                  # Transformer model components
â”œâ”€â”€ download_dataset.py
â”œâ”€â”€ config.py                     # Global config
â”œâ”€â”€ val_predictions.csv           # CSV containing predicted validation results
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ§  Model Architecture

- Transformer encoder (built from scratch)
- Multi-head self-attention
- Positional encoding
- Residual connections and layer normalization
- Classification head (linear + softmax)

## ğŸ§ª Training

- Optimizer: Adam
- Loss: CrossEntropy with label smoothing
- Dropout for regularization
- On-the-fly data augmentation for audio inputs (e.g., pitch/time shifting)

Training and validation performance are visualized in `figures/`.

## ğŸ“Š Evaluation

Confusion matrix and accuracy/loss curves can be found in:
- `figures/confusion_matrix.png`
- `figures/training_curves_cremad.png`
- `figures/data_dist_cremad.png`

## ğŸ™ï¸ Real-Time Inference

Run the following to test with live microphone input:

```bash
python inference.py
```

> Press `ESC` to stop recording loop.

## ğŸ§© Supported Emotions

For CREMA-D:  
`[angry, disgust, fearful, happy, neutral, sad]`

## âœ… Status

- âœ… Custom transformer encoder
- âœ… Dataset parsing & preprocessing
- âœ… On-the-fly audio augmentation
- âœ… Real-time microphone inference
- âœ… CREMA-D support (speaker-independent split)
- âœ… Visualization of metrics

## ğŸ“Œ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

## âœï¸ Author

Mehmet Utku Ã‡olak
